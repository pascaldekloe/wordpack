#include "textflag.h"

// func deltaDump(out *byte)
TEXT ·deltaDump(SB),NOSPLIT,$0
	MOVD	out+0(FP), R1
	// DEBUG: write to output pointer
	VST1.P	[V10.D2, V11.D2, V12.D2, V13.D2], (R1)
	VST1.P	[V14.D2, V15.D2, V16.D2, V17.D2], (R1)
	VST1.P	[V18.D2, V19.D2, V20.D2, V21.D2], (R1)
	VST1.P	[V22.D2, V23.D2, V24.D2, V25.D2], (R1)
	RET

// func delta64(in *int64, offset int64) (mask uint64)
// The deltas are zig-zag encoded into registers V10–V25.
TEXT ·delta64(SB),NOSPLIT,$0
	MOVD	in+0(FP), R0
	MOVD	offset+8(FP), R2

	// sign bit-mask into V9
	VMOVQ	$0x1000000000000000, $0x1000000000000000, V9
	// delta offset into V8
	VMOV	R2, V8.D[1] // direct from stack possible?

	// fetch first batch [half] into V0–V7
	VLD1.P	(R0), [V0.D2, V1.D2, V2.D2, V3.D2]
	VLD1.P	(R0), [V4.D2, V5.D2, V6.D2, V7.D2]
	// match previous value of V0–V7 into V10–V17
	VEXT	$8, V0.B16, V8.B16, V10.B16 // first agaist offset
	VEXT	$8, V1.B16, V0.B16, V11.B16
	VEXT	$8, V2.B16, V1.B16, V12.B16
	VEXT	$8, V3.B16, V2.B16, V13.B16
	VEXT	$8, V4.B16, V3.B16, V14.B16
	VEXT	$8, V5.B16, V4.B16, V15.B16
	VEXT	$8, V6.B16, V5.B16, V16.B16
	VEXT	$8, V7.B16, V6.B16, V17.B16
	// update offset for second batch
	VMOV	V7.D[1], V8.D[1]
	// calculate V0–V7 deltas into V10–17
	VSUB	V0.D2, V10.D2, V10.D2
	VSUB	V1.D2, V11.D2, V11.D2
	VSUB	V2.D2, V12.D2, V12.D2
	VSUB	V3.D2, V13.D2, V13.D2
	VSUB	V4.D2, V14.D2, V14.D2
	VSUB	V5.D2, V15.D2, V15.D2
	VSUB	V6.D2, V16.D2, V16.D2
	VSUB	V7.D2, V17.D2, V17.D2
	// zig-zag encode deltas
	VSHL	$1, V10.D2, V0.D2
	VSHL	$1, V11.D2, V1.D2
	VSHL	$1, V12.D2, V2.D2
	VSHL	$1, V13.D2, V3.D2
	VSHL	$1, V14.D2, V4.D2
	VSHL	$1, V15.D2, V5.D2
	VSHL	$1, V16.D2, V6.D2
	VSHL	$1, V17.D2, V7.D2
	// arithmetic shift 63 (workaround)
	VCMTST	V9.D2, V10.D2, V10.D2
	VCMTST	V9.D2, V11.D2, V11.D2
	VCMTST	V9.D2, V12.D2, V12.D2
	VCMTST	V9.D2, V13.D2, V13.D2
	VCMTST	V9.D2, V14.D2, V14.D2
	VCMTST	V9.D2, V15.D2, V15.D2
	VCMTST	V9.D2, V16.D2, V16.D2
	VCMTST	V9.D2, V17.D2, V17.D2
	// final merge of zig-zag into V10–V17
	VEOR	V10.B16, V0.B16, V10.B16
	VEOR	V11.B16, V1.B16, V11.B16
	VEOR	V12.B16, V2.B16, V12.B16
	VEOR	V13.B16, V3.B16, V13.B16
	VEOR	V14.B16, V4.B16, V14.B16
	VEOR	V15.B16, V5.B16, V15.B16
	VEOR	V16.B16, V6.B16, V16.B16
	VEOR	V17.B16, V7.B16, V17.B16

	// fetch second batch [half] into V0–V7
	VLD1.P	(R0), [V0.D2, V1.D2, V2.D2, V3.D2]
	VLD1.P	(R0), [V4.D2, V5.D2, V6.D2, V7.D2]
	// match previous value of V0–V7 into V18–V25
	VEXT	$8, V0.B16, V8.B16, V18.B16 // first agaist offset
	VEXT	$8, V1.B16, V0.B16, V19.B16
	VEXT	$8, V2.B16, V1.B16, V20.B16
	VEXT	$8, V3.B16, V2.B16, V21.B16
	VEXT	$8, V4.B16, V3.B16, V22.B16
	VEXT	$8, V5.B16, V4.B16, V23.B16
	VEXT	$8, V6.B16, V5.B16, V24.B16
	VEXT	$8, V7.B16, V6.B16, V25.B16
	// calculate V0–V7 deltas into V18–V25
	VSUB	V0.D2, V18.D2, V18.D2
	VSUB	V1.D2, V19.D2, V19.D2
	VSUB	V2.D2, V20.D2, V20.D2
	VSUB	V3.D2, V21.D2, V21.D2
	VSUB	V4.D2, V22.D2, V22.D2
	VSUB	V5.D2, V23.D2, V23.D2
	VSUB	V6.D2, V24.D2, V24.D2
	VSUB	V7.D2, V25.D2, V25.D2
	// zig-zag encode deltas V18–V25
	VSHL	$1, V18.D2, V0.D2
	VSHL	$1, V19.D2, V1.D2
	VSHL	$1, V20.D2, V2.D2
	VSHL	$1, V21.D2, V3.D2
	VSHL	$1, V22.D2, V4.D2
	VSHL	$1, V23.D2, V5.D2
	VSHL	$1, V24.D2, V6.D2
	VSHL	$1, V25.D2, V7.D2
	// arithmetic shift 63 (workaround)
	VCMTST	V9.D2, V18.D2, V18.D2
	VCMTST	V9.D2, V19.D2, V19.D2
	VCMTST	V9.D2, V20.D2, V20.D2
	VCMTST	V9.D2, V21.D2, V21.D2
	VCMTST	V9.D2, V22.D2, V22.D2
	VCMTST	V9.D2, V23.D2, V23.D2
	VCMTST	V9.D2, V24.D2, V24.D2
	VCMTST	V9.D2, V25.D2, V25.D2
	// final merge of zig-zag into V18–V25
	VEOR	V18.B16, V0.B16, V18.B16
	VEOR	V19.B16, V1.B16, V19.B16
	VEOR	V20.B16, V2.B16, V20.B16
	VEOR	V21.B16, V3.B16, V21.B16
	VEOR	V22.B16, V4.B16, V22.B16
	VEOR	V23.B16, V5.B16, V23.B16
	VEOR	V24.B16, V6.B16, V24.B16
	VEOR	V25.B16, V7.B16, V25.B16

	// bitmask 32 (zig-zag encoded) deltas V10–V25 into V26
	VORR	V10.B16, V11.B16, V26.B16
	VORR	V26.B16, V12.B16, V26.B16
	VORR	V26.B16, V13.B16, V26.B16
	VORR	V26.B16, V14.B16, V26.B16
	VORR	V26.B16, V15.B16, V26.B16
	VORR	V26.B16, V16.B16, V26.B16
	VORR	V26.B16, V17.B16, V26.B16
	VORR	V26.B16, V18.B16, V26.B16
	VORR	V26.B16, V19.B16, V26.B16
	VORR	V26.B16, V20.B16, V26.B16
	VORR	V26.B16, V21.B16, V26.B16
	VORR	V26.B16, V22.B16, V26.B16
	VORR	V26.B16, V23.B16, V26.B16
	VORR	V26.B16, V24.B16, V26.B16
	VORR	V26.B16, V25.B16, V26.B16
	VMOV	V26.D[0], R0
	VMOV	V26.D[1], R1
	ORR	R0, R1, R0
	MOVD	R0, mask+16(FP)
	RET

TEXT ·pack1(SB),NOSPLIT,$0
	MOVD	out+0(FP), R0
	VMOV	V25.B16, V0.B16
	VSLI	$1,  V24.D2, V0.D2
	VSLI	$2,  V23.D2, V0.D2
	VSLI	$3,  V22.D2, V0.D2
	VSLI	$4,  V21.D2, V0.D2
	VSLI	$5,  V20.D2, V0.D2
	VSLI	$6,  V19.D2, V0.D2
	VSLI	$7,  V18.D2, V0.D2
	VSLI	$8,  V17.D2, V0.D2
	VSLI	$9,  V16.D2, V0.D2
	VSLI	$10, V15.D2, V0.D2
	VSLI	$11, V14.D2, V0.D2
	VSLI	$12, V13.D2, V0.D2
	VSLI	$13, V12.D2, V0.D2
	VSLI	$14, V11.D2, V0.D2
	VSLI	$15, V10.D2, V0.D2
	VMOV	V0.H[4], V0.H[1]
	VST1	V0.S[0], (R0)
	RET

TEXT ·pack2(SB),NOSPLIT,$0
	MOVD	out+0(FP), R0
	VMOV	V25.B16, V0.B16
	VSLI	$2,   V24.D2, V0.D2
	VSLI	$4,   V23.D2, V0.D2
	VSLI	$6,   V22.D2, V0.D2
	VSLI	$8,   V21.D2, V0.D2
	VSLI	$10,  V20.D2, V0.D2
	VSLI	$12,  V19.D2, V0.D2
	VSLI	$14,  V18.D2, V0.D2
	VSLI	$16,  V17.D2, V0.D2
	VSLI	$18,  V16.D2, V0.D2
	VSLI	$20, V15.D2, V0.D2
	VSLI	$22, V14.D2, V0.D2
	VSLI	$24, V13.D2, V0.D2
	VSLI	$26, V12.D2, V0.D2
	VSLI	$28, V11.D2, V0.D2
	VSLI	$30, V10.D2, V0.D2
	VMOV	V0.S[2], V0.S[1]
	VST1	V0.D[0], (R0)
	RET
